{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/ishagupta/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/ishagupta/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/ishagupta/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/ishagupta/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/ishagupta/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/ishagupta/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "[nltk_data] Downloading package punkt to /Users/ishagupta/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ishagupta/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ishagupta/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!pip install wikipedia\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models.fasttext import FastText\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import WordPunctTokenizer\n",
    "\n",
    "import wikipedia\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data\n",
    "#### removing pucntuations\n",
    "#### removing numbers\n",
    "####  converting to lower case\n",
    "####  lemmatizing\n",
    "#### removing stop words\n",
    "####  lastly, removing words less than length of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(document):\n",
    "        # Remove all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    "\n",
    "        # remove all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        tokens = document.split()\n",
    "        tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "        tokens = [word for word in tokens if word not in en_stop]\n",
    "        tokens = [word for word in tokens if len(word) > 2]\n",
    "\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "\n",
    "        return preprocessed_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.5 s, sys: 3.02 s, total: 21.5 s\n",
      "Wall time: 19.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ft_model = FastText(word_tokenized_corpus,\n",
    "                      size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      sg=1,\n",
    "                      iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list=list()\n",
    "quote_tags=scraped.keys()\n",
    "for word in list(set(preprocess_text('black dog is running through the grass').split())):\n",
    "    for tuple in ft_model.wv.most_similar([word],topn=3):\n",
    "        my_list.append(tuple[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 60\n",
    "window_size = 40\n",
    "min_word = 5\n",
    "down_sampling = 1e-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.1 s, sys: 2.6 s, total: 35.7 s\n",
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ft_model = FastText(word_tokenized_corpus,\n",
    "                      size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      sg=1,\n",
    "                      iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras - 2 API - Sequential & functional\n",
    "The Embedding layer expects the words to be in numeric form. herefore, we need to convert the sentences in our corpus to numbers. One way to convert text to numbers is by using the one_hot function from the keras.preprocessing.text library. The function takes sentence and the total length of the vocabulary and returns the sentence in numeric form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method of pre-trained word embeddings - glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14,  3, 15, 16,  1,  0,  0,  0,  0,  0],\n",
       "       [ 4, 17,  6,  9,  5,  7,  2,  0,  0,  0],\n",
       "       [18, 19, 20,  2,  3, 21,  0,  0,  0,  0],\n",
       "       [22, 23,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 25, 26, 27,  5,  7,  2,  0,  0,  0],\n",
       "       [28,  8,  9, 29,  0,  0,  0,  0,  0,  0],\n",
       "       [30, 31, 32,  8, 33,  1,  0,  0,  0,  0],\n",
       "       [ 2,  3,  8, 34,  1,  0,  0,  0,  0,  0],\n",
       "       [10, 11,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [35, 36, 37,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [12, 38,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2,  6, 39, 40,  0,  0,  0,  0,  0,  0],\n",
       "       [ 5, 41, 13,  7,  4,  1,  0,  0,  0,  0],\n",
       "       [ 4,  1,  6, 10,  0,  0,  0,  0,  0,  0],\n",
       "       [ 5, 42, 13, 43,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4, 11,  3, 12,  0,  0,  0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenizer=Tokenizer()\n",
    "word_tokenizer.fit_on_texts(corpus)\n",
    "embedded=word_tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "pad_sequences(embedded,10,padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the advanced deep learning models involving multiple inputs and outputs use the Functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file=open('glove.6B/glove.6B.300d.txt',encoding=\"utf8\")\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "response = requests.get('https://www.quotery.com/topics/pain')\n",
    "results_page=BeautifulSoup(response.content,'lxml')\n",
    "#print(results_page.prettify())\n",
    "all_a_tags = results_page.find_all('a')\n",
    "div_tag = results_page.find('div')\n",
    "#div_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Users/ishagupta/anaconda3/lib/python3.7/site-packages (3.141.0)\r\n",
      "Requirement already satisfied: urllib3 in /Users/ishagupta/anaconda3/lib/python3.7/site-packages (from selenium) (1.25.8)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome(executable_path='/Users/ishagupta/Documents/MSBASPRING2020/chromedriver')\n",
    "\n",
    "pages=11\n",
    "scraped = dict()\n",
    "for i in range(1,pages):\n",
    "    url = 'http://quotes.toscrape.com/'\n",
    "    if i ==1:\n",
    "        url=url\n",
    "    else:\n",
    "        url=url+'/page/'+str(i)\n",
    "    driver.get(url)\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    quotes = soup.find_all('div', class_='quote')\n",
    "\n",
    "    for quote in quotes:\n",
    "    \n",
    "        text = quote.find('span', class_='text').text\n",
    "        author = quote.find('small', class_='author').text\n",
    "        for tags in quote.find('div', class_='tags').find_all('a'):\n",
    "            tag=tags.text\n",
    "            if scraped.get(tag) is None:\n",
    "                scraped[tag]=[[text,author]]\n",
    "            else:\n",
    "                scraped[tag].append([text,author])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = soup.find_all('div', class_='quote')\n",
    "scraped = dict()\n",
    "for quote in quotes:\n",
    "    \n",
    "    text = quote.find('span', class_='text').text\n",
    "    author = quote.find('small', class_='author').text\n",
    "    for tags in quote.find('div', class_='tags').find_all('a'):\n",
    "        tag=tags.text\n",
    "        if scraped.get(tag) is None:\n",
    "            scraped[tag]=[text,author]\n",
    "        else:\n",
    "            scraped[tag].append([text,author])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#pd.to_pickle(scraped,'scraped_quotes.pkl')\n",
    "scraped=pd.read_pickle('scraped_quotes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”',\n",
       " 'Albert Einstein']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped['change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['group', 'people', 'standing', 'front', 'audience']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wisdom : [['“Any fool can know. The point is to understand.”', 'Albert Einstein']]\n",
      "simile : [['“A day without sunshine is like, you know, night.”', 'Steve Martin'], ['“Life is like riding a bicycle. To keep your balance, you must keep moving.”', 'Albert Einstein'], [\"“He's like a drug for you, Bella.”\", 'Stephenie Meyer']]\n",
      "wander : [['“Not all those who wander are lost.”', 'J.R.R. Tolkien']]\n",
      "writers : [['“You have to write the book that wants to be written. And if the book will be too difficult for grown-ups, then you write it for children.”', \"Madeleine L'Engle\"]]\n",
      "hate : [[\"“The opposite of love is not hate, it's indifference. The opposite of art is not ugliness, it's indifference. The opposite of faith is not heresy, it's indifference. And the opposite of life is not death, it's indifference.”\", 'Elie Wiesel']]\n"
     ]
    }
   ],
   "source": [
    "a=preprocess_text('dog swims in the water')\n",
    "a.split()\n",
    "\n",
    "b=list()\n",
    "for word in a.split():\n",
    "    b.extend(difflib.get_close_matches(word, scraped.keys()))\n",
    "    \n",
    "for key in b:\n",
    "    print(key,':',scraped[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['change', 'deep-thoughts', 'thinking', 'world', 'abilities', 'choices', 'inspirational', 'life', 'live', 'miracle', 'miracles', 'aliteracy', 'books', 'classic', 'humor', 'be-yourself', 'adulthood', 'success', 'value', 'love', 'edison', 'failure', 'paraphrased', 'misattributed-eleanor-roosevelt', 'obvious', 'simile', 'friends', 'heartbreak', 'sisters', 'courage', 'simplicity', 'understand', 'fantasy', 'navigation', 'activism', 'apathy', 'hate', 'indifference', 'opposite', 'philosophy', 'friendship', 'lack-of-friendship', 'lack-of-love', 'marriage', 'unhappy-marriage', 'contentment', 'fate', 'misattributed-john-lennon', 'planning', 'plans', 'poetry', 'happiness', 'attributed-no-source', 'religion', 'comedy', 'yourself', 'children', 'fairy-tales', 'imagination', 'music', 'learning', 'reading', 'seuss', 'dumbledore', 'misattributed-to-mother-teresa', 'death', 'chocolate', 'food', 'misattributed-to-c-s-lewis', 'knowledge', 'understanding', 'wisdom', 'library', 'read', 'readers', 'reading-books', 'tea', 'girls', 'hope', 'attributed', 'fear', 'inspiration', 'thought', 'misattributed-to-einstein', 'drug', 'romance', 'novelist-quotes', 'alcohol', 'the-hunger-games', 'bilbo', 'journey', 'lost', 'quest', 'travel', 'wander', 'live-death-love', 'good', 'writing', 'regrets', 'education', 'troubles', 'open-mind', 'authors', 'literature', 'insanity', 'lies', 'lying', 'self-indulgence', 'truth', 'beatles', 'connection', 'dreamers', 'dreaming', 'dreams', 'peace', 'sinister', 'mistakes', 'romantic', 'women', 'integrity', 'elizabeth-bennet', 'jane-austen', 'age', 'fairytales', 'growing-up', 'god', 'misattributed-mark-twain', 'christianity', 'faith', 'sun', 'adventure', 'better-life-empathy', 'difficult', 'grown-ups', 'write', 'writers', 'mind'])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyDictionary\n",
      "  Downloading PyDictionary-1.5.2.zip (9.5 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/ishagupta/anaconda3/lib/python3.7/site-packages (from PyDictionary) (4.8.2)\n",
      "Collecting goslate\n",
      "  Downloading goslate-1.5.1.tar.gz (17 kB)\n",
      "Requirement already satisfied: requests in /Users/ishagupta/anaconda3/lib/python3.7/site-packages (from PyDictionary) (2.23.0)\n",
      "Requirement already satisfied: click in /Users/ishagupta/anaconda3/lib/python3.7/site-packages (from PyDictionary) (7.1.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /Users/ishagupta/anaconda3/lib/python3.7/site-packages (from beautifulsoup4->PyDictionary) (2.0)\n",
      "Collecting futures\n",
      "  Downloading futures-3.1.1-py3-none-any.whl (2.8 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/ishagupta/anaconda3/lib/python3.7/site-packages (from requests->PyDictionary) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ishagupta/anaconda3/lib/python3.7/site-packages (from requests->PyDictionary) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ishagupta/anaconda3/lib/python3.7/site-packages (from requests->PyDictionary) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ishagupta/anaconda3/lib/python3.7/site-packages (from requests->PyDictionary) (1.25.8)\n",
      "Building wheels for collected packages: PyDictionary, goslate\n",
      "  Building wheel for PyDictionary (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyDictionary: filename=PyDictionary-1.5.2-py3-none-any.whl size=4740 sha256=2c14dcbd8163513bfe7d5835b552b94017b4a9405720018fb170e83241b0a7f3\n",
      "  Stored in directory: /Users/ishagupta/Library/Caches/pip/wheels/0c/32/3e/fda11cb6426e5b61e55903e0ccae40c926ef194977001e307e\n",
      "  Building wheel for goslate (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for goslate: filename=goslate-1.5.1-py3-none-any.whl size=11549 sha256=defff776889325bb1b9170c3c7a25ceedd74b8182f39de7d07ad36e6984c3d43\n",
      "  Stored in directory: /Users/ishagupta/Library/Caches/pip/wheels/16/5c/86/f561bb944dc9f853f9502427bde8fe557a6fafe146d4a432f2\n",
      "Successfully built PyDictionary goslate\n",
      "Installing collected packages: futures, goslate, PyDictionary\n",
      "Successfully installed PyDictionary-1.5.2 futures-3.1.1 goslate-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install PyDictionary\n",
    "from PyDictionary import PyDictionary\n",
    "\n",
    "dictionary=PyDictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two', 'dog', 'running', 'snow']"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "scipy.spatial.distance.cosine([1,2,3],[1,2,4])\n",
    "key_words_prediction.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.linalg.norm(vec1-vec2)\n",
    "\n",
    "vec1=a['group']\n",
    "vec2=a['people']\n",
    "vec3=a['standing']\n",
    "sim1=0\n",
    "sim2=0\n",
    "sim3=0\n",
    "a1=''\n",
    "a2=''\n",
    "a3=''\n",
    "for word in list(new_scraped.keys()):\n",
    "    try:\n",
    "        vec=a[word]\n",
    "        if np.linalg.norm(vec1-a[word])>sim1:\n",
    "            sim1=np.linalg.norm(vec1-a[word])\n",
    "            a1=word\n",
    "        if np.linalg.norm(vec2-a[word])>sim2:\n",
    "            sim2=np.linalg.norm(vec2-a[word])\n",
    "            a2=word\n",
    "        #print(word,np.linalg.norm(vec3-a[word]))\n",
    "        if np.linalg.norm(vec3-a[word])>sim3:\n",
    "            sim3=np.linalg.norm(vec3-a[word])\n",
    "            a3=word\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9dd8eb5fd63e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'embeddings_dictionary.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "embeddings=pd.read_pickle('embeddings_dictionary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = 'boy is playing soccer'\n",
    "key_words_prediction = preprocess_text(prediction)\n",
    "key_word_embeddings=[embeddings[word] for word in key_words_prediction.split()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_tag=dict()\n",
    "sim=np.zeros(len(key_word_embeddings))\n",
    "for word in list(scraped.keys()):\n",
    "    try:\n",
    "        vec=embeddings[word]\n",
    "        for i in range(len(key_words_prediction)):\n",
    "\n",
    "            if 1-scipy.spatial.distance.cosine(key_word_embeddings[i],vec)>sim[i]:\n",
    "                sim[i]=1-scipy.spatial.distance.cosine(key_word_embeddings[i],vec)\n",
    "                closest_tag[key_words_prediction.split()[i]]=(sim[i],word)\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############  Choose one of the captions below: ##############\n",
      "\t\n",
      "\n",
      "\n",
      " “If you can make a woman laugh, you can make her do anything.” \t\tby Marilyn Monroe\n",
      "\t\n",
      "\n",
      "\n",
      " “There is nothing to writing. All you do is sit down at a typewriter and bleed.” \t\tby Ernest Hemingway\n",
      "\t\n",
      "\n",
      "\n",
      " “The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.” \t\tby Albert Einstein\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('###############  Choose one of the captions below: ##############')\n",
    "for tag,values in closest_tag.items():\n",
    "    print('\\t\\n\\n\\n',scraped[values[1]][0][0], '\\t\\tby',scraped[values[1]][0][1])\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'closest_tag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e95e9927153a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclosest_tag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'closest_tag' is not defined"
     ]
    }
   ],
   "source": [
    "closest_tag.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
